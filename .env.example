# Ollama API URL - Modify if you're running Ollama on a different host or port
OLLAMA_API_URL=http://localhost:11434

# The LLM model to use - Make sure this model is available in your Ollama installation
# Common options: llama2, mistral, codellama, deepseek, etc.
MODEL=deepseek-r1:1.5b

# Optional: Set CUDA_VISIBLE_DEVICES if you have multiple GPUs
# CUDA_VISIBLE_DEVICES=0 